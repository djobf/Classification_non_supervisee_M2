---
title: "DM1 Modèles à structure latente"
author: "Rajeeth Arumainathan, Jules Aubry, Youssef Ben Fadhel"
date: '2023-11-24'
output:
  html_document: default
  pdf_document: default
---

```{r,echo = FALSE,warning=FALSE}
library(ggplot2)
library(MASS)
library(gridExtra)
library(stats)
library(cluster)
data(crabs, package = "MASS")
```

# I) Données et ACP

## 1. Analyse exploratoire

Nous travaillons sur les données crabs. Les données contiennent 5 mesures de morphologies faites sur un échantillion de 200 crabes (taille du lobe frontal, largeur arrière, longueur de la carapace, largeur de la carapace et profondeur du corps). Les crabs sont divisées en 4 classes, réparties selon l'espèce (bleue ou orange) et le sexe. L'objectif serait d'imaginer une étude de clustering qui ne les utiliserait pas et par la suite de comparer la classification obtenue avec celles en espèce et en sexe.

```{r,echo = FALSE}
plots_list <- list()
for (variable in c("FL", "RW", "CL", "CW", "BD")) {
  p <- ggplot(crabs, aes(x = !!as.symbol(variable))) +
    geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribution de", variable),
         x = variable,
         y = "Fréquence") +
    theme_minimal() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
  
  plots_list[[variable]] <- p
}
grid.arrange(grobs = plots_list, ncol = 2)
```

L'ensemble des variables semblent avoir une distribution normale. On ne peut pas comparer les 200 crabes graphiquement en fonction des 5 mesures de morphologies car il faudrait un graphique en 5 dimensions et l'oeil humain s'arrête à 3 dimensions. Une réduction de dimension s'impose donc, pour cela on va utiliser l'ACP. L'ACP permet de réduire le nombre de variables d'un ensemble de données en projetant ces variables dans un nouvel espace de dimensions inférieures. L'objectif est de projeter les données dans un nouvel espace où la première composante principale explique autant de variances que possible, de même pour la deuxième composante principale et ainsi de suite.

## 2. Etude de l'ACP

### 2.a. Proportion de variance expliquée par les deux premières composantes

```{r,echo = FALSE}
df_crabs <- crabs[, c("FL", "RW", "CL", "CW", "BD")]
df_crabs <- scale(df_crabs) #On standardise pour éviter que les variables avec des unités de mesure différentes aient un poids disproportionné dans l'analyse.
results <- prcomp(df_crabs, scale = TRUE)
results$rotation <- -1*results$rotation
results$sdev^2 / sum(results$sdev^2)
```

La première composante principale explique 95,77% de la variance des données ainsi elles expliquent la majorité de la variance des données. En comparaisons, la deuxième composante principale n'explique que 3% de la variance des données ce qui paraît minime.

### 1.b. Projection des données sur leur première composante principale

```{r,echo = FALSE}
plot(results$x[, 1], rep(0, length(results$x[, 1])),
     main = "Projection unidimensionnelle",
     xlab = "Première Composante Principale",
     ylab = "",
     pch = 16, col = "blue",yaxt = "n")
```

Aucun groupe ne se dégage de la projection des données sur leur première composante principale

### 1.c. Projection des données sur leur deuxième composante principale

```{r,echo = FALSE}
plot(results$x[, 2], rep(0, length(results$x[, 2])),
     main = "Projection unidimensionnelle",
     xlab = "Deuxième Composante Principale",
     ylab = "",
     pch = 16, col = "blue",yaxt = "n")
```

Deux groupes semblent se dégager de la projection des données sur leur deuxième composante principale.

### 1.d. Projection des données sur leurs deux premières composantes principales.

```{r,echo = FALSE}
plot(results$x[, 1],results$x[, 2],
     main = "Projection bidimensionnelle",
     xlab = "Première Composante Principale",
     ylab = "Deuxième Composante Principale",
     pch = 16, col = "blue")

```

Deux groupes semblent également se dégager de la projection des données sur leurs deux premières composantes principales, un au-dessus et un en dessous.

## 3. Conclusion

L'ACP ne semble pas former 4 groupes distincs mais seulement 2 pour les données crabs, en effet il semblerait qu'on ne garde pas assez d'information sur la répartition des données pour les séparer en 4 groupes. Voyons ce que cela donne en affichant le sexe et l'espèce

```{r,echo = FALSE}
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], sex = crabs$sex, sp = crabs$sp),aes(x = PC1, y = PC2, color = interaction(factor(sex), factor(sp)))) +
  geom_point(size = 3) +
  labs(title = "Projection des données sur PC1 et PC2",
       x = "Première Composante Principale (PC1)",
       y = "Deuxième Composante Principale (PC2)",
       color = "Sexe et espèce")
```

En effet l'ACP a seulement séparé les mâles et les femelles, cependant les distinctions entre les espèces sont faites par leur couleur or l'ACP ne prend pas en compte cette donnée. On peut donc supposer que l'ACP reste efficace dans le cadre des données continues.

# II) Kmeans et clustering hiérarchique ascendant

On travaille à présent sur les données seeds, elles concernent des mesures de graines de trois variétés différentes de blé (Kama, Rosa et Canadian) :

```{r,echo = FALSE}
seeds <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt", header = FALSE)
colnames(seeds) <- c("Aire", "Périmètre", "Compacité", "Longueur", "Largeur", "Asymétrie", "Longueur rainure", "Class")
plots_list <- list()
for (variable in c("Aire", "Périmètre", "Compacité", "Longueur", "Largeur", "Asymétrie", "Longueur rainure")) {
  p <- ggplot(seeds, aes(x = !!as.symbol(variable))) +
    geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribution de", variable),
         x = variable,
         y = "Fréquence") +
    theme_minimal() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
  
  plots_list[[variable]] <- p
}
grid.arrange(grobs = plots_list, ncol = 3)

```

La plupart des variables semblent collées à la distribution de la loi normale. Regardons la répartition des données à travers l'ACP :

```{r,echo = FALSE}
df_seeds <- seeds[, c("Aire", "Périmètre", "Compacité", "Longueur", "Largeur", "Asymétrie", "Longueur rainure")]
df_seeds <- scale(df_seeds) #On standardise pour éviter que les variables avec des unités de mesure différentes aient un poids disproportionné dans l'analyse.
results <- prcomp(df_seeds, scale = TRUE)
plot(results$x[, 1],results$x[, 2],
     main = "Projection bidimensionnelle",
     xlab = "Première Composante Principale",
     ylab = "Deuxième Composante Principale",
     pch = 16, col = "blue")


```

L'ACP semble mettre en évidence 3 groupes, un à gauche, un en bas et un à droite.

## 1. Kmeans

On va maintenant utiliser la méthode des Kmeans pour obtenir une première classification. Pour cela il faut faire attention au nombre de cluster K que l'on souhaite (ici 3) mais on peut également faire attention au nombre maximal d'itérations pour éviter une exécution infinie.

```{r,echo = FALSE}
variables_kmeans <- seeds[, c("Aire", "Périmètre", "Compacité", "Longueur", "Largeur", "Asymétrie", "Longueur rainure")]
kmeans_result <- kmeans(variables_kmeans, centers = 3)
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(kmeans_result$cluster)),aes(x = PC1, y = PC2, color = Cluster)) +geom_point(size = 3) +labs(title ="ACP",x = "PC1",y = "PC2",color = "Cluster")
```

L'algorithme des k-means séparent en effet les données comme les 3 groupes que nous avions distingués. Regardons les coefficients de silhouette moyens pour chaque cluster :

```{r,echo = FALSE}
aggregate(silhouette(kmeans_result$cluster, dist(variables_kmeans))[, "sil_width"], by=list(Cluster=kmeans_result$cluster), FUN=mean)$x
```

Les coefficients de silhouette moyens sont relativement élevés pour chaque cluster, les points sont en moyenne beaucoup plus proches de leur groupe que du groupe voisin. Regardons la somme des carrés intra et inter-cluster:

```{r,echo = FALSE}
paste("Intra-cluster",sum(kmeans_result$withinss))
paste("Inter-cluster",sum(kmeans_result$betweenss))
```

L'inertie inter-classe est élevée par rapport à l'inertie intra-classe, les clusters sont bien séparés les uns des autres. Il peut être intéressant de représenter graphiquement l'évolution des affectations des classes et des centres de celles-ci au cours des itérations de l'algorithme :

```{r,echo = FALSE,warning=FALSE}
plots_list <- list()
for (i in 1:3) {
  set.seed(123)  # Fixer la graine aléatoire pour la reproductibilité
  kmeans_result <- kmeans(variables_kmeans, centers = 3, iter.max = i)
  pca_centers <- prcomp(scale(kmeans_result$centers))
  centers_df <- data.frame(PC1 = pca_centers$x[, 1], PC2 = pca_centers$x[, 2])
  
  plots_list[[i]] <-ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(kmeans_result$cluster)),aes(x = PC1, y = PC2, color = Cluster)) + geom_point(size = 3) +geom_point(data = centers_df, aes(x = PC1, y = PC2), color = "black", size = 3, alpha = 0.5) +labs(title = paste("Itération : ",i),x = "PC1",y = "PC2",color = "Cluster")
}
grid.arrange(grobs = plots_list, ncol = 2)

```

On constate que les clusters évoluent au fil des itérations et que leur centre se déplace, en effet les nouveaux centres de chaque nouveau cluster sont calculés en prenant la moyenne des points attribués pour chacun d'entre eux.

## 2. Clustering hiérarchique ascendant

On va maintenant regarder avec le clustering hiérarchique ascendant. On va pour cela considérer différentes fonctions de linkage (Ward, single linkage, complete linkage,group average). Commençons par Ward :

### Ward

```{r,echo = FALSE}
dist_matrix <- dist(seeds[, 1:7])
hierarchical_result <- hclust(dist_matrix, method = "ward.D2")
num_clusters <- 3
hierarchical_clusters <- cutree(hierarchical_result, k = num_clusters)
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(hierarchical_clusters)),aes(x = PC1, y = PC2, color = Cluster)) +geom_point(size = 3) +labs(title ="Fonction Ward",x = "PC1",y = "PC2",color = "Cluster")


```

La fonction Ward favorise la formation de clusters homogènes en minimisant la variance intra-cluster. Les classes choisies semblent similaires à celle choisie par la méthode des k-means.

```{r,echo = FALSE}
aggregate(silhouette(hierarchical_clusters, dist(variables_kmeans))[, "sil_width"], by=list(Cluster=hierarchical_clusters), FUN=mean)$x
```

Les coefficients de silhouette moyens sont légèrement inférieurs à la méthode des k-means mais restent relativement élevé pour chaque cluster, les points sont en moyenne beaucoup plus proche de leur groupe que du groupe voisin.

```{r,echo = FALSE}
paste("Intra-cluster",sum(hierarchical_result$height[hierarchical_clusters]))
paste("Inter-cluster",sum(hierarchical_result$height)-sum(hierarchical_result$height[hierarchical_clusters]))
```

L'inertie inter-classe est élevée par rapport à l'inertie intra-classe, les clusters sont bien séparés les uns des autres.

### Single linkage

```{R,echo = FALSE}
hierarchical_result <- hclust(dist_matrix, method = "single")
  num_clusters <- 3
  hierarchical_clusters <- cutree(hierarchical_result, k = num_clusters)
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(hierarchical_clusters)),aes(x = PC1, y = PC2, color = Cluster)) +geom_point(size = 3) +labs(title ="single",x = "PC1",y = "PC2",color = "Cluster")
```

La fonction single linkage semblent avoir créé une très grande classe et deux petites, en effet elle a tendance à former des clusters étendus.

```{r,echo = FALSE}
aggregate(silhouette(hierarchical_clusters, dist(variables_kmeans))[, "sil_width"], by=list(Cluster=hierarchical_clusters), FUN=mean)$x
```

Les coefficients de silhouette moyens sont très élevés pour le deuxième et troisième cluster, en revanche il est très proche de 0 pour le premier cluster, les points sont aussi proche de leur groupe que du groupe voisin pour le premier cluster.

```{r,echo = FALSE}
paste("Intra-cluster",sum(hierarchical_result$height[hierarchical_clusters]))
paste("Inter-cluster",sum(hierarchical_result$height)-sum(hierarchical_result$height[hierarchical_clusters]))
```

L'inertie inter-classe reste élevé par rapport à l'inertie intra-classe, les clusters sont bien séparés les uns des autres.

### Complete linkage

```{r,echo = FALSE}
hierarchical_result <- hclust(dist_matrix, method = "complete")
  num_clusters <- 3
  hierarchical_clusters <- cutree(hierarchical_result, k = num_clusters)
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(hierarchical_clusters)),aes(x = PC1, y = PC2, color = Cluster)) +geom_point(size = 3) +labs(title ="complete",x = "PC1",y = "PC2",color = "Cluster")

```

La fonctions complete linkage donne des classes similaires aux méthode ward et k-means, on peut l'expliquer par le fait qu'elle est moins sujette à la formation de clusters étendus.

```{r,echo = FALSE}
aggregate(silhouette(hierarchical_clusters, dist(variables_kmeans))[, "sil_width"], by=list(Cluster=hierarchical_clusters), FUN=mean)$x
```

Les coefficients de silhouette moyens sont relativement élevés pour chaque cluster, les points sont en moyenne beaucoup plus proches de leur groupe que du groupe voisin.

```{r,echo = FALSE}
paste("Intra-cluster",sum(hierarchical_result$height[hierarchical_clusters]))
paste("Inter-cluster",sum(hierarchical_result$height)-sum(hierarchical_result$height[hierarchical_clusters]))
```

L'inertie inter-classe est élevée par rapport à l'inertie intra-classe, les clusters sont bien séparés les uns des autres.

### group average

```{r,echo = FALSE}
hierarchical_result <- hclust(dist_matrix, method = "average")
  num_clusters <- 3
  hierarchical_clusters <- cutree(hierarchical_result, k = num_clusters)
ggplot(data.frame(PC1 = results$x[, 1], PC2 = results$x[, 2], Cluster =as.factor(hierarchical_clusters)),aes(x = PC1, y = PC2, color = Cluster)) +geom_point(size = 3) +labs(title ="average",x = "PC1",y = "PC2",color = "Cluster")

```

La fonctions group average donne des classes similaires aux méthodes précédentes, on peut l'expliquer par le fait qu'elle est moins sujette à la formation de clusters étendus.

```{r,echo = FALSE}
aggregate(silhouette(hierarchical_clusters, dist(variables_kmeans))[, "sil_width"], by=list(Cluster=hierarchical_clusters), FUN=mean)$x
```

Les coefficients de silhouette moyens sont relativement élevés pour chaque cluster, les points sont en moyenne beaucoup plus proches de leur groupe que du groupe voisin.

```{r,echo = FALSE}
paste("Intra-cluster",sum(hierarchical_result$height[hierarchical_clusters]))
paste("Inter-cluster",sum(hierarchical_result$height)-sum(hierarchical_result$height[hierarchical_clusters]))
```

L'inertie inter-classe est élevée par rapport à l'inertie intra-classe, les clusters sont bien séparés les uns des autres.

## 3. Conclusion

La majorité des méthodes donnent des résultats satisfaisants, à l'exception de single linkage. On retrouve à chaque fois les 3 variétés de blé espéré. Ceci valide donc l'efficacité des différentes méthodes de classification.
